{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **Download Llama2 7b and expose it through Fast API application**"
      ],
      "metadata": {
        "id": "4U0ENasANUqi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU0GYiR4hWKJ"
      },
      "source": [
        "## Install Dependencies\n",
        "- Requirements for running FastAPI Server\n",
        "- Requirements for creating a public model serving URL via Ngrok\n",
        "- Requirements for running Llama2 7B (including Quantization)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDVaaatLEpzq"
      },
      "outputs": [],
      "source": [
        "# Build Llama cpp\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u24fGvOmT2Pi"
      },
      "outputs": [],
      "source": [
        "# If this complains about dependency resolver, it's safe to ignore\n",
        "!pip install fastapi[all] uvicorn python-multipart transformers pydantic tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kxx3xpr2cXHS"
      },
      "outputs": [],
      "source": [
        "# This downloads and sets up the Ngrok executable in the Google Colab instance\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -o ngrok-stable-linux-amd64.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPOYhvtAHiW3"
      },
      "source": [
        "Ngrok is used to make the FastAPI server accessible via a public URL.\n",
        "\n",
        "Users are required to make a free account and provide their auth token to use Ngrok. The free version only allows 1 local tunnel and the auth token is used to track this usage limit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nw3LWCfNcg9E"
      },
      "outputs": [],
      "source": [
        "# https://dashboard.ngrok.com/signup\n",
        "!./ngrok authtoken 2XoGoHZUYMKGuTAM13OsnCMVTKn_2hysrPmkmSuDfjEeBCKgn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUqG0_-6IChq"
      },
      "source": [
        "## Create FastAPI App\n",
        "This provides an API to the Llama 2 model. The model version can be changed in the code below as desired.\n",
        "\n",
        "For this demo we will use the 7 billion parameter version which is finetuned for instruction (chat) following."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6F078OHVssf"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "from typing import Any\n",
        "\n",
        "from fastapi import FastAPI\n",
        "from fastapi import HTTPException\n",
        "from pydantic import BaseModel\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# GGML model required to fit Llama2-13B on a T4 GPU\n",
        "# GENERATIVE_AI_MODEL_REPO = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "# GENERATIVE_AI_MODEL_FILE = \"llama-2-13b-chat.ggmlv3.q5_1.bin\"\n",
        "\n",
        "GENERATIVE_AI_MODEL_REPO = \"TheBloke/Llama-2-7B-chat-GGUF\"\n",
        "GENERATIVE_AI_MODEL_FILE = \"llama-2-7b-chat.Q6_K.gguf\"\n",
        "\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=GENERATIVE_AI_MODEL_REPO,\n",
        "    filename=GENERATIVE_AI_MODEL_FILE\n",
        ")\n",
        "\n",
        "llama2_model = Llama(\n",
        "    model_path=model_path,\n",
        "    n_gpu_layers=64,\n",
        "    n_ctx=2000\n",
        ")\n",
        "\n",
        "# Test an inference\n",
        "print(llama2_model(prompt=\"Hello \", max_tokens=1))\n",
        "\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "\n",
        "# This defines the data json format expected for the endpoint, change as needed\n",
        "class TextInput(BaseModel):\n",
        "    inputs: str\n",
        "    parameters: dict[str, Any] | None\n",
        "\n",
        "\n",
        "@app.get(\"/\")\n",
        "def status_gpu_check() -> dict[str, str]:\n",
        "    gpu_msg = \"Available\" if tf.test.is_gpu_available() else \"Unavailable\"\n",
        "    return {\n",
        "        \"status\": \"I am ALIVE!\",\n",
        "        \"gpu\": gpu_msg\n",
        "    }\n",
        "\n",
        "\n",
        "@app.post(\"/generate/\")\n",
        "async def generate_text(data: TextInput) -> dict[str, str]:\n",
        "    try:\n",
        "        params = data.parameters or {}\n",
        "\n",
        "        stuff = data.inputs\n",
        "\n",
        "        prompt = f'''Please generate a two line rhyme based on this headline -> {stuff}'''\n",
        "        prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
        "\n",
        "        USER: {prompt}\n",
        "\n",
        "        ASSISTANT: Sure, here is a two line rhyme for you:\n",
        "        '''\n",
        "\n",
        "        response = llama2_model(prompt=prompt_template, **params)\n",
        "        model_out = response['choices'][0]['text']\n",
        "        return {\"generated_text\": model_out}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PikyQUQKIewj"
      },
      "source": [
        "## Start FastAPI Server\n",
        "The initial run will take a long time due to having to download the model and load it onto GPU.\n",
        "\n",
        "Note: interrupting the Google Colab runtime will send a SIGINT and stop the server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HErEYVtPcGg9"
      },
      "outputs": [],
      "source": [
        "# This cell finishes quickly because it just needs to start up the server\n",
        "# The server will start the model download and will take a while to start up\n",
        "# ~5 minutes\n",
        "!uvicorn app:app --host 0.0.0.0 --port 8000 > server.log 2>&1 &"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tunHZmTHX98z"
      },
      "source": [
        "Check the logs at server.log to see progress.\n",
        "\n",
        "Wait until model is loaded and check with the next cell before moving on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJUJ4vICfQ7a"
      },
      "outputs": [],
      "source": [
        "# If you see \"Failed to connect\", it's because the server is still starting up\n",
        "# Wait for the model to be downloaded and the server to fully start\n",
        "# Check the server.log file to see the status\n",
        "!curl localhost:8000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csO2FA7LKWXw"
      },
      "source": [
        "## Use Ngrok to create a public URL for the FastAPI server.\n",
        "**IMPORTANT:** If you created an account via email, please verify your email or the next 2 cells won't work.\n",
        "\n",
        "If you signed up via Google or GitHub account, you're good to go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "c-FpVEkifNdS"
      },
      "outputs": [],
      "source": [
        "# This starts Ngrok and creates the public URL\n",
        "from IPython import get_ipython\n",
        "get_ipython().system_raw('./ngrok http 8000 &')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxzFtg3_Kfot"
      },
      "source": [
        "Check the URL generated by the next cell, it should report that the FastAPI server is alive and that GPU is available.\n",
        "\n",
        "To hit the model endpoint, simply add `/generate` to the URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCAIkVuxc3JU"
      },
      "outputs": [],
      "source": [
        "# Get the Public URL\n",
        "# If this doesn't work, make sure you verified your email\n",
        "# Then run the previous code cell and this one again\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liqVEsGfZPse"
      },
      "source": [
        "## Shutting Down\n",
        "To shut down the processes, run the following commands in a new cell:\n",
        "```\n",
        "!pkill uvicorn\n",
        "!pkill ngrok\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}